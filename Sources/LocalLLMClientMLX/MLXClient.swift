import LocalLLMClientCore
import MLX
import MLXLMCommon
import Foundation

/// A client for interacting with MLX models.
///
/// This actor-based class provides methods for generating text streams from various inputs,
/// and handles the communication with the underlying MLX model via the `MLX` and `MLXLMCommon` frameworks.
public final actor MLXClient: LLMClient {
    let context: Context
    let parameter: MLXClient.Parameter
    let tools: [AnyLLMTool]
    let pauseHandler: PauseHandler

    /// Initializes a new MLX client.
    ///
    /// - Parameters:
    ///   - url: The URL of the MLX model directory. This directory should contain the model weights, tokenizer configuration, and any other necessary model files.
    ///   - parameter: The parameters for the MLX model. Defaults to `.default`.
    ///   - tools: Optional array of tools that can be used by the model for function calling.
    /// - Throws: An error if the client fails to initialize, for example, if the model files cannot be loaded.
    nonisolated public init(url: URL, parameter: Parameter = .default, tools: [any LLMTool] = []) async throws {
        context = try await Context(url: url, parameter: parameter)
        self.parameter = parameter
        self.tools = tools.map { AnyLLMTool($0) }
        self.pauseHandler = PauseHandler(disableAutoPause: parameter.options.disableAutoPause)
    }

    /// Generates a text stream from the given input.
    ///
    /// This function processes the input, whether it's plain text, a chat template, or structured chat messages,
    /// and prepares it for the MLX model. It then generates text asynchronously.
    ///
    /// - Parameter input: The input to generate text from. This can be plain text, a chat template, or an array of chat messages.
    /// - Returns: An `AsyncStream<String>` that yields text chunks as they are generated by the model.
    /// - Throws: An `LLMError.visionUnsupported` error if the input contains images and the loaded model does not support vision.
    ///           It can also throw errors related to model processing or input preparation.
    public func textStream(from input: LLMInput) async throws -> AsyncStream<String> {
        try await context.modelContainer.perform { @Sendable context in
            let chat = input.chatMessages

            var userInput = UserInput(chat: chat, additionalContext: ["enable_thinking": false]) // TODO: public API
            userInput.processing.resize = .init(width: 448, height: 448)

            if chat.contains(where: { !$0.images.isEmpty }), !self.context.supportsVision {
                throw LLMError.visionUnsupported
            }


            let lmInput = try await context.processor.prepare(input: userInput)
            let stream = try MLXLMCommon.generate(
                input: lmInput,
                parameters: parameter.parameters,
                context: context
            )

            return .init { continuation in
                let task = Task {
                    for await generated in stream {
                        await pauseHandler.checkPauseState()
                        continuation.yield(generated.chunk ?? "")
                    }
                    continuation.finish()
                }
                continuation.onTermination = { _ in
                    task.cancel()
                }
            }
        }
    }
    
    /// Converts tool parameters to MLX format
    func convertParametersToMLXFormat(_ parameters: [String: any Sendable]) -> [ToolParameter] {
        guard let properties = parameters["properties"] as? [String: [String: any Sendable]] else {
            return []
        }
        
        let required = parameters["required"] as? [String] ?? []

        return properties.compactMap { key, value in
            guard let type = value["type"] as? String,
                  let description = value["description"] as? String else {
                return nil
            }
            
            let mlxType = convertToToolParameterType(from: type, value: value)
            
            var extraProperties: [String: any Sendable] = [:]
            if let enumValues = value["enum"] as? [String] {
                extraProperties["enum"] = enumValues
            }
            
            if required.contains(key) {
                return ToolParameter.required(key, type: mlxType, description: description, extraProperties: extraProperties.isEmpty ? [:] : extraProperties)
            } else {
                return ToolParameter.optional(key, type: mlxType, description: description, extraProperties: extraProperties.isEmpty ? [:] : extraProperties)
            }
        }
    }
    
    private func convertToToolParameterType(from type: String, value: [String: any Sendable]) -> ToolParameterType {
        switch type {
        case "string":
            return .string
        case "integer", "number":
            return .int
        case "boolean":
            return .bool
        case "array":
            return convertArrayType(from: value)
        case "object":
            return convertObjectType(from: value)
        default:
            return .string
        }
    }
    
    private func convertArrayType(from value: [String: any Sendable]) -> ToolParameterType {
        guard let items = value["items"] as? [String: any Sendable],
              let itemType = items["type"] as? String else {
            return .array(elementType: .string)
        }
        
        let elementType: ToolParameterType
        switch itemType {
        case "string":
            elementType = .string
        case "integer":
            elementType = .int
        case "number":
            elementType = .double
        case "boolean":
            elementType = .bool
        case "object":
            // For array of objects, parse the object schema
            if let objectProperties = items["properties"] as? [String: [String: any Sendable]] {
                let mlxProperties = convertParametersToMLXFormat([
                    "properties": objectProperties as [String: any Sendable],
                    "required": (items["required"] as? [String] ?? []) as [String] as any Sendable
                ])
                elementType = .object(properties: mlxProperties)
            } else {
                elementType = .object(properties: [])
            }
        default:
            elementType = .string
        }
        
        return .array(elementType: elementType)
    }
    
    private func convertObjectType(from value: [String: any Sendable]) -> ToolParameterType {
        if let objectProperties = value["properties"] as? [String: [String: any Sendable]] {
            let mlxProperties = convertParametersToMLXFormat([
                "properties": objectProperties as [String: any Sendable],
                "required": (value["required"] as? [String] ?? []) as [String] as any Sendable
            ])
            return .object(properties: mlxProperties)
        } else {
            return .object(properties: [])
        }
    }
    
    /// Streams responses from the input
    /// - Parameter input: The input to process
    /// - Returns: An asynchronous sequence that emits response content (text chunks, tool calls, etc.)
    /// - Throws: An error if generation fails
    public func responseStream(from input: LLMInput) async throws -> AsyncThrowingStream<StreamingChunk, any Error> {
        return AsyncThrowingStream { continuation in
            Task {
                do {
                    // Convert tools to MLX schema format
                    let toolSchemas = tools.map { tool in
                        Tool<EmptyInput, EmptyOutput>(
                            name: tool.name,
                            description: tool.description,
                            parameters: convertParametersToMLXFormat(tool.argumentsSchema)
                        ) { _ in EmptyOutput() }.schema
                    }

                    let stream = try await context.modelContainer.perform { @Sendable (context: ModelContext) -> AsyncStream<Generation> in
                        let chat = input.chatMessages

                        if chat.contains(where: { !$0.images.isEmpty }), !self.context.supportsVision {
                            throw LLMError.visionUnsupported
                        }

                        var innerLocalUserInput = UserInput(
                            chat: chat,
                            tools: toolSchemas.isEmpty ? nil : toolSchemas,
                            additionalContext: ["enable_thinking": false]
                        )
                        innerLocalUserInput.processing.resize = CGSize(width: 448, height: 448)

                        let lmInput = try await context.processor.prepare(input: innerLocalUserInput)
                        return try MLXLMCommon.generate(
                            input: lmInput,
                            parameters: parameter.parameters,
                            context: context
                        )
                    }
                    
                    for try await generation in stream {
                        await pauseHandler.checkPauseState()
                        switch generation {
                        case let .chunk(text):
                            continuation.yield(.text(text))
                        case .info: ()
                        case let .toolCall(toolCall):
                            // Convert MLX ToolCall to LLMToolCall
                            let arguments = try JSONEncoder().encode(toolCall.function.arguments)

                            let llmToolCall = LLMToolCall(
                                id: UUID().uuidString, // Generate ID since MLX ToolCall doesn't have one
                                name: toolCall.function.name,
                                arguments: String(decoding: arguments, as: UTF8.self)
                            )
                            continuation.yield(.toolCall(llmToolCall))
                        }
                    }
                    
                    continuation.finish()
                } catch {
                    continuation.finish(throwing: error)
                }
            }
        }
    }
    
    /// Pauses any ongoing text generation
    public func pauseGeneration() async {
        await pauseHandler.pause()
    }
    
    /// Resumes previously paused text generation
    public func resumeGeneration() async {
        await pauseHandler.resume()
    }
    
    /// Whether the generation is currently paused
    public var isGenerationPaused: Bool {
        get async {
            await pauseHandler.isPaused
        }
    }
}

// Empty types for parameter conversion
private struct EmptyInput: Codable {}
private struct EmptyOutput: Codable {}


public extension LocalLLMClient {
    /// Creates a new MLX client.
    ///
    /// This is a factory method for creating `MLXClient` instances.
    ///
    /// - Parameters:
    ///   - url: The URL of the MLX model directory. This directory should contain the model weights, tokenizer configuration, and any other necessary model files.
    ///   - parameter: The parameters for the MLX model. Defaults to `.default`.
    ///   - tools: Optional array of tools that can be used by the model for function calling.
    /// - Returns: A new `MLXClient` instance.
    /// - Throws: An error if the client fails to initialize, for example, if the model files cannot be loaded.
    static func mlx(url: URL, parameter: MLXClient.Parameter = .default, tools: [any LLMTool] = []) async throws -> MLXClient {
        try await MLXClient(url: url, parameter: parameter, tools: tools)
    }
}
